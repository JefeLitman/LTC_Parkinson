{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select the GPU ID to work or -1 to CPU: -1\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "#@title Initial config for notebooks { display-mode: \"form\" }\n",
    "import os\n",
    "from getpass import getpass\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = input('Select the GPU ID to work or -1 to CPU: ')\n",
    "\n",
    "# Auto detecting Colab or server:\n",
    "if os.getcwd() == \"/content\":\n",
    "    os.environ[\"SERVER\"] = \"0\"\n",
    "else:\n",
    "    os.environ[\"SERVER\"] = \"1\"\n",
    "\n",
    "if int(os.getenv('SERVER')):\n",
    "    !git pull\n",
    "else:\n",
    "    # Import the encoder function to URL's\n",
    "    import urllib.parse\n",
    "    # Delete folders in /content/\n",
    "    for folder in os.listdir('/content/'):\n",
    "        if folder == \"drive\":\n",
    "            raise ValueError('You have the drive folder mounted, reset the '\n",
    "                'the machine to fabric state to work again.')\n",
    "        else:\n",
    "            os.system(\"rm -rf /content/\"+folder)\n",
    "    # User credentials\n",
    "    os.environ[\"USER\"] = input('Github username: ')\n",
    "    os.environ[\"PASS\"] = urllib.parse.quote(getpass('Password: '))\n",
    "    # Py archives\n",
    "    !git clone \"https://$USER:$PASS@github.com/JefeLitman/LTC_Parkinson.git\" ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "This module is not intended to work in the local server because the data is already in it. Just find it ;)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fc0606af5ad9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mv DatasetsLoaderUtils.py utils/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetsLoaderUtils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mflow_from_tablePaths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mltc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/LTC_Parkinson/utils/download_data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SERVER\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     raise NotImplementedError(\"This module is not intended to work in the local \"\n\u001b[0m\u001b[1;32m     23\u001b[0m         \"server because the data is already in it. Just find it ;)\")\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: This module is not intended to work in the local server because the data is already in it. Just find it ;)"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import gc\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "!wget -q https://raw.githubusercontent.com/JefeLitman/VideoDataGenerator/master/DatasetsLoaderUtils.py -O DatasetsLoaderUtils.py\n",
    "!mv DatasetsLoaderUtils.py utils/\n",
    "from utils.DatasetsLoaderUtils import flow_from_tablePaths\n",
    "from utils import download_data\n",
    "from models import ltc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial configuration of tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getenv(\"CUDA_VISIBLE_DEVICES\") != '-1':\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "\n",
    "if not int(os.getenv('SERVER')):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not int(os.getenv('SERVER')):\n",
    "    download_data.parkinson_gait_cutted_zip()\n",
    "ori_path = '/home/jefelitman/DataSets/Parkinson_cutted' #'/content/data/data2020'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation of experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/home/jefelitman/Saved_Models/Parkinson/LTC_SGD_IC_Scaled/'\n",
    "if not os.path.isdir(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "epoch_x_patient = 10\n",
    "\n",
    "readme = open(os.path.join(save_path, \"README.txt\"), \"w+\")\n",
    "readme.write(\n",
    "    \"\"\"This file contains information about the experiment made in this instance.\n",
    "    \n",
    "All models saved doesn't include the optimizer, but this file explains how to train in the same conditions.\n",
    "\n",
    "Classic settings:\n",
    "- The seed used was 8128 for python random module, numpy random and tf random for before the instance of every patient's model.\n",
    "- The batch size was of 2.\n",
    "- This experiment use the methodology leave one patient out.\n",
    "- The initial lr and weight decay was of 0.001 and 0.005 respectively.\n",
    "- Every model was trained with 10 epochs.\n",
    "- The optimizer to train was SGD with a momentum of 0.9.\n",
    "- In training process we decreased the lr and weight decay reduce at 40% and 80% of epoch by a factor of 0.1.\n",
    "- The networks have a dropout of 0.5.\n",
    "\n",
    "Basic notation and Symbols:\n",
    "- <model> (LTC): Name of the model, in this experiment was LTC.\n",
    "- Init Controlled (IC): Set the seeds for tensorflow, numpy and random modules.\n",
    "- BatchNormalization (BN): A batch normalization module applied.\n",
    "- scale (Scaled): The data is scaled before passing through the network.\n",
    "- <optimizer> (SGD): Optimizer used to train, in this experiment was SGD.\n",
    "\n",
    "Transformations over the data:\n",
    "- The LTC model used was the version 1.0\n",
    "- All the data come from Colombian Parkinson cutted dataset (data2020.zip) \n",
    "- Scaled: Divide each pixel of video in all frames by 255.\n",
    "- time_sampling: Take the half frames of videos, the select the range between half-30 -> half+30 frames.\n",
    "\"\"\"\n",
    ")\n",
    "readme.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cambio_lr(i, lr):\n",
    "    if i == int(epoch_x_patient*0.4) or i == int(epoch_x_patient*0.8) :\n",
    "        for i in ['conv3d_1','conv3d_2','conv3d_3','conv3d_4','conv3d_5', 'dense_6', 'dense_7', 'dense_8']:\n",
    "            weigh_decay = model.get_layer(i).kernel_regularizer.get_config()['l2'] * 0.1\n",
    "            model.get_layer(i).kernel_regularizer = keras.regularizers.l2(weigh_decay)\n",
    "        return model.optimizer.learning_rate.numpy() * 0.1\n",
    "    else:\n",
    "        return model.optimizer.learning_rate.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (Leave one patient out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "# Creando los datos para cada paciente\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "for i, j in [('CG','01'),('PG','01'),('CG','02'),('PG','02'),('CG','03'),('PG','03'),\n",
    "             ('CG','04'),('PG','04'),('CG','05'),('PG','05'),('CG','06'),('PG','06'),\n",
    "             ('CG','07'),('PG','07'),('CG','08'),('PG','08'),('CG','09'),('PG','09'),\n",
    "             ('CG','10'),('PG','10'),('CG','11'),('PG','11')]:\n",
    "    \n",
    "    random.seed(8128)\n",
    "    np.random.seed(8128)\n",
    "    tf.random.set_seed(8128)\n",
    "\n",
    "    save_path_patient = save_path + i+'_'+j\n",
    "    if not os.path.isdir(save_path_patient):\n",
    "        os.mkdir(save_path_patient)\n",
    "    \n",
    "    table_paths = []\n",
    "    for video in sorted(os.listdir(ori_path)):\n",
    "        video_path = os.path.join(ori_path,video)\n",
    "        if video.split(\"_\")[0] == i and video.split(\"_\")[1] == j:\n",
    "            table_paths.append([video_path, 'test', video.split(\"_\")[0]])\n",
    "        else:\n",
    "            table_paths.append([video_path, 'train', video.split(\"_\")[0]])\n",
    "\n",
    "    def time_sampling(video):\n",
    "        mitad = len(video)//2\n",
    "        return video[mitad-30:mitad+30]\n",
    "    \n",
    "    def min_max_scale(video, label):\n",
    "        x_min = tf.reduce_min(video, name=\"Min_video\")\n",
    "        x_max = tf.reduce_max(video, name=\"Max_video\")\n",
    "        new_video = tf.divide((video - x_min), (x_max - x_min), name=\"Scaling_video\")\n",
    "        return new_video, label\n",
    "    \n",
    "    def scale(video, label):\n",
    "        return video/255., label\n",
    "    \n",
    "    dataset = flow_from_tablePaths(table_paths, time_sampling, [256, 256])\n",
    "    len_train = len(dataset.__videos_train_path__)\n",
    "    len_test = len(dataset.__videos_test_path__)\n",
    "    \n",
    "    train_data = tf.data.Dataset.from_generator(dataset.data_generator, \n",
    "                                                (tf.float32, tf.int64), \n",
    "                                                ([60, 256, 256, 1], []), \n",
    "                                                args=[1, 1])\n",
    "    train_data = train_data.cache().shuffle(len_train, \n",
    "                                            reshuffle_each_iteration=True).map(scale, -1).batch(2).prefetch(-1)\n",
    "    \n",
    "    test_data = tf.data.Dataset.from_generator(dataset.data_generator, \n",
    "                                                (tf.float32, tf.int64), \n",
    "                                                ([60, 256, 256, 1], []), \n",
    "                                                args=[2, 1])\n",
    "    test_data = test_data.cache().shuffle(len_test, \n",
    "                                          reshuffle_each_iteration=True).map(scale, -1).batch(2).prefetch(-1)\n",
    "\n",
    "    #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    # Entrenamiento de la red neuronal\n",
    "    #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    name_weights = \"final_model_patient_\" + i + '_' + j + \"_weights.h5\"\n",
    "    clear_output()\n",
    "    print(\"Listo! Modelo que se va salvar: \", name_weights)\n",
    "    model = ltc.get_LTC_v1_0((60, 256, 256, 1), 0.5, 2, 5e-3)\n",
    "    #Parametros de la red neuronal\n",
    "    sgd = tf.keras.optimizers.SGD(learning_rate=1e-3, momentum=0.9)\n",
    "    perdida = tf.keras.losses.SparseCategoricalCrossentropy(name='loss')\n",
    "    precision = tf.keras.metrics.SparseCategoricalAccuracy(name='acc')\n",
    "    model.compile(optimizer=sgd, loss = perdida, metrics = [precision])\n",
    "    model_cinematic = model.fit(x = train_data,\n",
    "                  epochs=epoch_x_patient,\n",
    "                  callbacks = [tf.keras.callbacks.LearningRateScheduler(cambio_lr, verbose=1),\n",
    "                               tf.keras.callbacks.CSVLogger(os.path.join(save_path_patient,i+'_'+j+'_output.csv'))],\n",
    "                  validation_data = test_data)\n",
    "    model.save(os.path.join(save_path_patient,name_weights), include_optimizer=False, save_format=\"h5\")\n",
    "    #Grafico la perdida\n",
    "    fig = plt.figure()\n",
    "    plt.plot(model_cinematic.history[\"loss\"],'k--')\n",
    "    plt.plot(model_cinematic.history[\"val_loss\"],'b--')\n",
    "    plt.title('Loss over epochs')\n",
    "    plt.legend(labels=[\"Loss\",\"Test Loss\"])\n",
    "    fig.savefig(os.path.join(save_path_patient,'train_loss.png'))\n",
    "    plt.close(fig)\n",
    "    #Grafico el accuracy\n",
    "    fig = plt.figure()\n",
    "    plt.plot(model_cinematic.history[\"acc\"],'k--')\n",
    "    plt.plot(model_cinematic.history[\"val_acc\"],'b--')\n",
    "    plt.title('Accuracy over epochs')\n",
    "    plt.legend(labels=[\"Accuracy\",\"Test Accuracy\"])\n",
    "    fig.savefig(os.path.join(save_path_patient,'train_accuracy.png'))\n",
    "    plt.close(fig)\n",
    "\n",
    "    #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    # Libero la memoria de la grafica\n",
    "    #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    del model\n",
    "    del dataset\n",
    "    del train_data\n",
    "    del test_data\n",
    "    del table_paths\n",
    "    del fig\n",
    "    del model_cinematic\n",
    "    del sgd\n",
    "    del perdida\n",
    "    del precision\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Upload your changes { display-mode: \"form\" }\n",
    "if not int(os.getenv('SERVER')):\n",
    "    !git config --global user.email \"$USER@github.com\"\n",
    "    !git config --global user.name \"$USER\"\n",
    "    # Deleting the .tfrecord files in the path\n",
    "    for file in os.listdir('/content/'):\n",
    "        if folder == \"data2020\":\n",
    "            os.system(\"rm -rf /content/\"+file)\n",
    "!git add -A *\n",
    "os.environ[\"COMMIT\"] = input(\"Insert the name for your changes: \")\n",
    "!git commit -m  \"$COMMIT\"\n",
    "!git push"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
